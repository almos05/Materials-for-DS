{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "gradient descent-v1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh-59psvhw7Z"
      },
      "source": [
        "# Градиентный спуск\n",
        "\n",
        "###  Степик Академия. Математика для Data Science.\n",
        "\n",
        "\n",
        "На первый взгляд задание может показаться объемным, но на самом деле почти весь код уже написан. Вам осталось только переписать строчки с комментарием: \n",
        "\n",
        "`# < ВАШ КОД: замените 0 на нужное выражение > `\n",
        "\n",
        "А также позапускать градиентый спуск с разными параметрами в разделе Эксперименты.\n",
        "\n",
        "### Начало работы с ноутбуком\n",
        "\n",
        "Первым шагом в этом задании сделайте копию ноутбука: File -> Save a copy in Drive или что-то аналогичное в русском интерфейсе. Ноутбук, который открывается по ссылке на Степике, имеет права только на чтение. Копия, которую вы создадите, будет доступна только вам и ее можно будет редактировать.\n",
        "\n",
        "### Важный комментарий\n",
        "\n",
        "Задание можно сделать, меняя только раздел Эксперименты, а также строчки с пометкой \n",
        "\n",
        "`# < ВАШ КОД: замените 0 на нужное выражение > `\n",
        "\n",
        "Конечно, мы не запрещаем вам менять вам другие места в коде — экспериментируйте. Но в этом случае очень важно понимать, какие изменения могут привести к тому, что ваш ответ будет отличаться от того, который проверяется в шаге на Степике в этом задании."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBTNfmMSjfKG"
      },
      "source": [
        "## Импортируем нужные библиотеки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8Cbbr80hw7b"
      },
      "source": [
        "from random import uniform\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMZURWBHhw7h"
      },
      "source": [
        "### Визуализации\n",
        "\n",
        "В этот код можете не заглядывать, он просто рисует красивые картинки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Med0m9PGhw7i"
      },
      "source": [
        "def plot_surface(func):\n",
        "    x = np.linspace(0, 100, 100).reshape(1, -1)\n",
        "    y = np.linspace(0, 100, 100).reshape(1, -1)\n",
        "    f = np.array([[func(x_i, y_j) for y_j in y[0]] for x_i in x[0]])\n",
        "\n",
        "    layout = go.Layout(scene=dict(aspectmode='cube'))\n",
        "\n",
        "\n",
        "    fig = go.Figure(data=[go.Surface(z=f)], layout=layout, )\n",
        "\n",
        "    legend=dict(font=dict(size=12), x=0.45, y=0.95,)\n",
        "\n",
        "    fig.update_layout(autosize=True,\n",
        "                      width=800, height=500,\n",
        "                      margin=dict(l=0, r=50, b=100, t=0),\n",
        "                      legend=legend,\n",
        "                      )\n",
        "    \n",
        "    return fig\n",
        "\n",
        "    \n",
        "def plot_gradient_descent_steps(fig, x_steps, y_steps, func_steps):\n",
        "    steps = go.Scatter3d(x=x_steps,\n",
        "                             y=y_steps,\n",
        "                             z=func_steps,\n",
        "                             mode='lines+markers',\n",
        "                             marker=dict(\n",
        "                                        size=5,\n",
        "                                        color='red',\n",
        "                                        colorscale='Viridis',\n",
        "                             ),\n",
        "                            line=dict(\n",
        "                                color='red',\n",
        "                                width=2\n",
        "                            )\n",
        "                        )\n",
        "    fig.add_trace(steps)\n",
        "    fig.show()\n",
        "    return steps\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcUhbOubhw7m"
      },
      "source": [
        "## Целевая функция\n",
        "\n",
        "Чтобы получить ответы, которые проверяются в шаге на Степике, эту функцию менять не нужно.\n",
        "\n",
        "Если есть желание, вы можете написать свою `func_2`, `func_3`, и тд,  чтобы поэкспериментировать с ними. Но эта часть не обязательная и никак не проверяется. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0MPjqfohw7n"
      },
      "source": [
        "def func_1(x, y):\n",
        "    # смещаем начало координат в точку (50, 50). чтобы поверхность отображалась в нужном месте\n",
        "    x -= 50\n",
        "    y -= 50\n",
        "    \n",
        "    return x ** 2 + y ** 2\n",
        "\n",
        "plot_surface(func_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXzzPyNPhw7p"
      },
      "source": [
        "### Вычислим градиент\n",
        "\n",
        "\n",
        "Сейчас большинство библиотек для машинного обучения, реализуют автоматическое дифферецирование. Однако, мы его использовать не будем, а напишем свою простую функцию, приближенно вычисляющую градиент в точке.\n",
        "\n",
        "**Комментарий 1.** У вас может возникнуть желание написать в этом месте частную производную конкретной `func_1`, которую мы определили выше. Однако в этом задании предлагается реализовать `gradient_finite_diff` таким образом, чтобы она не зависела от функции f, которая передается в аргментов. Достичь этого можно, вычисляя значения `df_dx` и `df_dy` по формуле аналогичной общей формуле частых производных, в которых используются конечные приращения `dx` и `dy`.\n",
        "\n",
        "**Комментарий 2.** Начиная с этого момента рекомендуем внимательно читать весь код: ваших знаний достаточно, чтобы понять его полностью."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v53Vlg4hw7q"
      },
      "source": [
        "# в аргументах передаются:\n",
        "#\n",
        "#     f -- функция зависящая от двух переменных\n",
        "#     x0, y0 -- точка, в которой требуется вычислить градиент\n",
        "#\n",
        "# опционально можно указать:\n",
        "#     dx, dy -- дельты по направлениям\n",
        "\n",
        "def gradient_finite_diff(f, x0, y0, dx=0.0001, dy=0.0001):\n",
        "    # частная производная по направлению оси х\n",
        "    df_dx = 0 # < ВАШ КОД: замените 0 на нужное выражение > \n",
        "    \n",
        "    # частная производная по направлению оси у\n",
        "    df_dy = 0 # < ВАШ КОД: замените 0 на нужное выражение > \n",
        "    \n",
        "    return (df_dx, df_dy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FJyXuBdhw7t"
      },
      "source": [
        "### Градиентный спуск\n",
        "\n",
        "Аналогично, в реализации `gradient_descent` не нужно привязываться к конкретному примеру, который мы рассматриваем. Напишите общие формулы для следующего шага в градиентом спуске. Аргументы `f`, `x0`, `y0` и тд передаются в функцию извне. Чтобы получить ответы для шага на Степике, в разделе Эксперименты вызовите функции с необходимыми значением этих параметров. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEI0v4D4hw7t"
      },
      "source": [
        "eps = 0.001\n",
        "\n",
        "def grad_norm(grad):\n",
        "    return (grad[0] ** 2 + grad[1] ** 2) ** (1/2)\n",
        "\n",
        "\n",
        "# в аргументах передаются:\n",
        "#\n",
        "#     f -- функция зависящая от двух переменных\n",
        "#\n",
        "# опционально можно указать:\n",
        "#\n",
        "#     (x0, y0) -- точка, с которой будет стартовать градиентный спуск, по умолчанию выбирается случайная точка\n",
        "#     learning_rate -- подробнее об этом в соответствующих уроках\n",
        "#     max_steps_number -- максимальное число шагов градиентного спуска\n",
        "#     plot -- рисовать ли график\n",
        "\n",
        "\n",
        "def gradient_descent(f,\n",
        "                     x0=uniform(0, 100),\n",
        "                     y0=uniform(0, 100),\n",
        "                     learning_rate=0.1,\n",
        "                     max_steps_number=10000,\n",
        "                     plot=True):\n",
        "    # рисуем поверхность\n",
        "    if plot:\n",
        "        fig = plot_surface(f)\n",
        "    \n",
        "    # cтартуем с (x0, y0)\n",
        "    step_number = 0\n",
        "    x = x0\n",
        "    y = y0\n",
        "    \n",
        "    # сохраняем последовательность наших шагов\n",
        "    steps_x = [x0]\n",
        "    steps_y = [y0]\n",
        "    steps_func = [f(x0, y0)]\n",
        "    \n",
        "    # критерий остановки градиентного спуска:\n",
        "    #        либо градиент становится близким к 0\n",
        "    #        либо сделано максимальное число шагов\n",
        "    while step_number < max_steps_number and grad_norm(gradient_finite_diff(f, x, y)) > eps:\n",
        "        # вычисляем градиент (получаем вектор с двумя компонентами, он понадобится в шаге градиентного спуска)\n",
        "        grad = gradient_finite_diff(f, x, y)\n",
        "        \n",
        "        # шаг градиентного спуска\n",
        "        x -= 0 # < ВАШ КОД: замените 0 на нужное выражение > \n",
        "        y -= 0 # < ВАШ КОД: замените 0 на нужное выражение > \n",
        "\n",
        "        # добавляем новую точку и значение в ней в список шагов\n",
        "        steps_x.append(x)\n",
        "        steps_y.append(y)\n",
        "        steps_func.append(f(x, y))\n",
        "        \n",
        "        step_number += 1\n",
        "    \n",
        "    if plot:\n",
        "        plot_gradient_descent_steps(fig, steps_x, steps_y, steps_func)\n",
        "        \n",
        "    if len(steps_x) == max_steps_number:\n",
        "        print(f\"gradient descent reached maximum number of steps, which is set to {max_steps_number}\")\n",
        "    else:\n",
        "        print(f\"gradient descent terminated after {(step_number + 1)} steps\")\n",
        "\n",
        "    print(f\"terminal point of gradient descent is (%.10f, %.10f) with function value %.10f\" % (steps_x[-1], steps_y[-1], steps_func[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMD0oX6Zhw7x"
      },
      "source": [
        "## Эксперименты\n",
        "\n",
        "Поэкспериментируйте с learning rate, начальной точкой.\n",
        "\n",
        "Чтобы получить ответы на вопросы в шаге на Степике, используйте значние `max_steps_number` по умолчанию. Оно влияет на критерий остановки, поэтому с другими значниями ваши ответы могут не сойтись с нашими."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2ZvEkQZhw7x"
      },
      "source": [
        "gradient_descent(func_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clJLMyR6opYJ"
      },
      "source": [
        "gradient_descent(func_1, learning_rate=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFnl8sgNpMDu"
      },
      "source": [
        "gradient_descent(func_1, learning_rate=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}